<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Note down something</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.pirrla.cn/"/>
  <updated>2017-07-12T06:31:50.000Z</updated>
  <id>http://www.pirrla.cn/</id>
  
  <author>
    <name>Alan Wong</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Elasticsearch相关性</title>
    <link href="http://www.pirrla.cn/2017/07/12/Elasticsearch%E7%9B%B8%E5%85%B3%E6%80%A7/"/>
    <id>http://www.pirrla.cn/2017/07/12/Elasticsearch相关性/</id>
    <published>2017-07-11T16:00:00.000Z</published>
    <updated>2017-07-12T06:31:50.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ElasticSearch-相关性"><a href="#ElasticSearch-相关性" class="headerlink" title="ElasticSearch 相关性"></a>ElasticSearch 相关性</h1><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h3><p>Elasticsearch版本：5.4.2</p>
<p>Kibana版本：5.4.2</p>
<p>Logstash版本：5.4.2</p>
<p>Elasticsearch在执行搜索后，返回结果的排序是根据字段<strong>_score</strong>决定的。排序结果是倒序排列，得分越高，越在前面。</p>
<p><strong>_score</strong>字段即量化表明查询语句与文档内容的相似性与匹配度，为了更好地显示搜索结果，我们需要深入了解该数值的来源与相关算法，本文讨论的相关性则为此。</p>
<h3 id="2-语句"><a href="#2-语句" class="headerlink" title="2.语句"></a>2.语句</h3><p>本文通过调用<strong>_search</strong>的<strong>explain</strong>参数进行调试。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">GET /_search?explain</div><div class="line">&#123;</div><div class="line">  "query":&#123;</div><div class="line">    "match":&#123;</div><div class="line">      "title":"a"</div><div class="line">  &#125;</div><div class="line"> &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="3-经典TF-IDF算法"><a href="#3-经典TF-IDF算法" class="headerlink" title="3.经典TF/IDF算法"></a>3.经典TF/IDF算法</h3><p>在Elasticsearch 5.X 版本前，Elasticsearch默认使用的是经典的TF/IDF算法，即：</p>
<p><strong>检索词频率（TermFrequency）</strong></p>
<p>检索词在该字段出现的频率？出现频率越高，相关性也越高。 字段中出现过 5 次要比只出现过 1 次的相关性高。当然，词频不是在score计算中直接使用的。通常是取词频（TermFrequency）的平方根。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">TF_Score = sqrt(termFreq)</div></pre></td></tr></table></figure>
<p><strong>反向文档频率（InverseDocumentFrequency）</strong></p>
<p>每个检索词在索引中出现的频率？频率越高，相关性越低。检索词出现在多数文档中会比出现在少数文档中的权重更低。例如中文中，“的”、“得”，“不”等词因在大多数文档中出现，所以相关性较低。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">IDF_Score = log(maxDocs/docFreq+1)+1</div></pre></td></tr></table></figure>
<p><strong>字段长度准则（FieldNorms）</strong></p>
<p>字段的长度是多少？长度越长，相关性越低。 检索词出现在一个短的 title 要比同样的词出现在一个长的 content 字段权重更大。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">FieldNorms = 1 / sqrt(length)</div></pre></td></tr></table></figure>
<p>所以最终得分为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Score = TF_Score*IDF_Score*FieldNorms</div><div class="line">      = log(maxDocs / (docFreq + 1)) * sqrt(termFreq) * (1/sqrt(length))</div></pre></td></tr></table></figure>
<p>*maxDocs有可能会数到已删除文档！</p>
<h3 id="4-BM25算法"><a href="#4-BM25算法" class="headerlink" title="4.BM25算法"></a>4.BM25算法</h3><p>BM25是Elasticsearch5.X的默认相关性算法，它根据TF/IDF算法改进而来。它由两部分组成，分别是IDF和TFNorm.</p>
<p>其中，根据不同的检索方法，数值各不相同，由于Elasticsearch是分布式系统，所以默认的检索方法为Query_Then_Search（该方法为在各自分片内进行检索，然后各分片返回各自的搜索结果至协调分片，非全局检索，当数据量充足时，效果良好，但当数据量较少时，应使用全局检索，请参看后文DFS_Query_Then_Search方法）</p>
<p><strong>单词检索</strong>（即检索内容只包含单个英语单词（空格间隔）或单个中文分词（由Tokenizer划分））：</p>
<p>IDF</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">log(1 + (docCount - docFreq + 0.5) / (docFreq + 0.5))</div></pre></td></tr></table></figure>
<p>TFNorm</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">termfreq * (k1 + 1)) / (termfreq + k1 * (1 - b + b * fieldLength / avgFieldLength)</div></pre></td></tr></table></figure>
<ul>
<li>docCount - 该索引该分片的文档总数</li>
<li>docFreq - 该索引内该分片内，含有检索内容的文档的数量</li>
<li>termFreq - 包含该检索内容的文档的检索内容出现次数</li>
<li>k1 &amp; b - BM25参数，常分别设为 1.2 和 0.75</li>
<li>fieldLength - 包含该检索内容的文档的长度</li>
<li>avgFieldLength - 该索引该分片的平均文档长度</li>
</ul>
<p><strong>多词检索</strong></p>
<p>如果检索内容为一段话或多个词语，首先通过Analyzer将一段话或多个词语进行筛选和划分，包括大小写转换、停用词去除、词根筛选、同义词转换、分词（英文根据空格划分，中文根据Tokenizer划分，本文使用中文分词工具ik）。然后将得到的多个词语逐个进行单词检索，然后最终得分为多个词语的得分的总和。</p>
<h3 id="5-其他算法"><a href="#5-其他算法" class="headerlink" title="5.其他算法"></a>5.其他算法</h3><p>如果需要更换算法，可以在创建索引的时候，添加settings设置，更改默认的相关性算法。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">PUT /test</div><div class="line">&#123;</div><div class="line">  "settings": &#123;</div><div class="line">    "index": &#123;</div><div class="line">      "similarity": &#123;</div><div class="line">        "default": &#123;</div><div class="line">          "type": "classic"</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>可选算法有</p>
<ul>
<li>“BM25” - 如无设置算法，则默认为BM25算法</li>
<li>“classic” - 经典TF/IDF算法</li>
</ul>
<p>除了以上的两种算法，其他的算法需要<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-similarity.html#lm_dirichlet" target="_blank" rel="external">配置参数</a></p>
<ul>
<li>“<a href="http://lucene.apache.org/core/5_2_1/core/org/apache/lucene/search/similarities/DFRSimilarity.html" target="_blank" rel="external">DFR</a>“  / “<a href="http://trec.nist.gov/pubs/trec21/papers/irra.web.nb.pdf" target="_blank" rel="external">DFI</a>“  / “<a href="http://lucene.apache.org/core/5_2_1/core/org/apache/lucene/search/similarities/IBSimilarity.html" target="_blank" rel="external">IB</a>“ / “<a href="http://lucene.apache.org/core/5_2_1/core/org/apache/lucene/search/similarities/LMDirichletSimilarity.html" target="_blank" rel="external">LMDirichlet</a>“ / “<a href="http://lucene.apache.org/core/5_2_1/core/org/apache/lucene/search/similarities/LMJelinekMercerSimilarity.html" target="_blank" rel="external">LMJelinekMercer</a>“</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">"similarity" : &#123;</div><div class="line">  "my_similarity" : &#123;</div><div class="line">    "type" : "DFR",</div><div class="line">    "basic_model" : "g",</div><div class="line">    "after_effect" : "l",</div><div class="line">    "normalization" : "h2",</div><div class="line">    "normalization.h2.c" : "3.0"</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>再在索引中设置mapping以应用算法</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="attr">"book"</span> : &#123;</div><div class="line">    <span class="attr">"properties"</span> : &#123;</div><div class="line">      <span class="attr">"title"</span> : &#123; <span class="attr">"type"</span> : <span class="string">"text"</span>, <span class="attr">"similarity"</span> : <span class="string">"my_similarity"</span> &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="6-Function-Score"><a href="#6-Function-Score" class="headerlink" title="6.Function Score"></a>6.Function Score</h3><p>待续</p>
<h3 id="7-同义词"><a href="#7-同义词" class="headerlink" title="7.同义词"></a>7.同义词</h3><p>使用同义词进行检索时，部分数据会发生变化，以BM25算法为例：</p>
<p>b,c为同义词</p>
<ul>
<li>docCount - 文档总数不变</li>
<li>docFreq - 所有包含b或c的文档的数量</li>
<li>termFreq - 如有一次b或c出现，则算双倍次数</li>
<li>k1 &amp; b - 不变</li>
<li>fieldLength - 不变</li>
<li>avgFieldLength - 每个b或c算双倍大小</li>
</ul>
<h3 id="8-DFS-Query-Then-Fetch"><a href="#8-DFS-Query-Then-Fetch" class="headerlink" title="8.DFS Query Then Fetch"></a>8.DFS Query Then Fetch</h3><p>可以全局搜索，当数据量较少时可以使用</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">GET /_search?search_type=dfs_query_then_fetch</div></pre></td></tr></table></figure>
<h3 id="资料参考："><a href="#资料参考：" class="headerlink" title="资料参考："></a>资料参考：</h3><p>[1] <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/relevance-intro.html" target="_blank" rel="external">Elasticsearch文档-什么是相关性？</a></p>
<p>[2] <a href="http://opensourceconnections.com/blog/2015/10/16/bm25-the-next-generation-of-lucene-relevation/" target="_blank" rel="external">BM25 The Next Generation of Lucene Relevance</a></p>
<p>[3] <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-similarity.html" target="_blank" rel="external">Elasticsearch文档-Similarity module</a></p>
<p>[4] <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html" target="_blank" rel="external">Elasticsearch文档-Function Score Query</a></p>
<p>[5] <a href="https://www.elastic.co/blog/understanding-query-then-fetch-vs-dfs-query-then-fetch" target="_blank" rel="external">Elasticsearch文档-DFS-DFS Query Then Fetch</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;ElasticSearch-相关性&quot;&gt;&lt;a href=&quot;#ElasticSearch-相关性&quot; class=&quot;headerlink&quot; title=&quot;ElasticSearch 相关性&quot;&gt;&lt;/a&gt;ElasticSearch 相关性&lt;/h1&gt;&lt;h3 id=&quot;1-简介&quot;
    
    </summary>
    
      <category term="elasticsearch" scheme="http://www.pirrla.cn/categories/elasticsearch/"/>
    
    
      <category term="elasticsearch" scheme="http://www.pirrla.cn/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>Mastering Social Media Mining wiht Python</title>
    <link href="http://www.pirrla.cn/2017/03/17/Mastering_Social_Media_Mining_with_Python/"/>
    <id>http://www.pirrla.cn/2017/03/17/Mastering_Social_Media_Mining_with_Python/</id>
    <published>2017-03-16T16:00:00.000Z</published>
    <updated>2017-03-25T17:59:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>Resource: <a href="https://marcobonzanini.com/2016/08/02/mastering-social-media-mining-with-python/" target="_blank" rel="external">Mastering Social Media Mining wiht Python</a></p>
<p>By Marco Bonzanini</p>
<h2 id="Python-tools-for-data-science"><a href="#Python-tools-for-data-science" class="headerlink" title="Python tools for data science"></a>Python tools for data science</h2><p>Pandas</p>
<p>Numpy</p>
<p>Scikit-learn</p>
<p>NLTK</p>
<p>Gensim</p>
<p>Pyro4</p>
<p>Networks</p>
<p>Matplotlib</p>
<p>wordcloud</p>
<h2 id="Twitter"><a href="#Twitter" class="headerlink" title="Twitter"></a>Twitter</h2><h2 id="Entity-Analysis"><a href="#Entity-Analysis" class="headerlink" title="Entity Analysis"></a>Entity Analysis</h2><p>1.Count the hashtags frenquency</p>
<p>2.Count the tweets with hashtags or without hashtags and count the number of hashtags in one tweet</p>
<p>3.Count the occurancy of people mention</p>
<h2 id="Text-Analysis"><a href="#Text-Analysis" class="headerlink" title="Text Analysis"></a>Text Analysis</h2><p>1.lowercase the texts</p>
<p>2.Tokenization</p>
<p> use NLTK.TweetTokenizer() to </p>
<p>3.remove the stop words</p>
<p>4.remove digits</p>
<p> Count the occrance of words</p>
<h2 id="Time-Series-Analysis"><a href="#Time-Series-Analysis" class="headerlink" title="Time Series Analysis"></a>Time Series Analysis</h2><p>Plot the figure with different timeslot</p>
<p>Normalize the time</p>
<p>For example from 15:13-15:14 all of them add to together</p>
<h2 id="Text-analysis-and-TF-IDF-on-notes"><a href="#Text-analysis-and-TF-IDF-on-notes" class="headerlink" title="Text analysis and TF-IDF on notes"></a>Text analysis and TF-IDF on notes</h2><p>TF = Term Frequency</p>
<p>frequency in a document, can be normalized by the length of the document</p>
<p>IDF = Inverse Document Frequency</p>
<p>1+log(N/n)  N means the total number of words, the n mean this word’s occrance in each document</p>
<p>TF-IDF = TF*IDF</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Resource: &lt;a href=&quot;https://marcobonzanini.com/2016/08/02/mastering-social-media-mining-with-python/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Maste
    
    </summary>
    
      <category term="CUHK_Notes" scheme="http://www.pirrla.cn/categories/CUHK-Notes/"/>
    
    
      <category term="data_mining" scheme="http://www.pirrla.cn/tags/data-mining/"/>
    
  </entry>
  
  <entry>
    <title>Git Tutorial</title>
    <link href="http://www.pirrla.cn/2017/03/16/GitTutorial_ByLiaoxuefeng/"/>
    <id>http://www.pirrla.cn/2017/03/16/GitTutorial_ByLiaoxuefeng/</id>
    <published>2017-03-15T16:00:00.000Z</published>
    <updated>2017-03-25T18:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>Source:</p>
<p><a href="http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="external">http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000</a></p>
<h1 id="Git-Tutorial"><a href="#Git-Tutorial" class="headerlink" title="Git Tutorial"></a>Git Tutorial</h1><p>Download:</p>
<p>For Mac user, you can simply go to the offical website to download the package and install it.</p>
<p>Link:<a href="https://git-scm.com/downloads" target="_blank" rel="external">https://git-scm.com/downloads</a></p>
<p>在终端里面，使用</p>
<p>pwd</p>
<p>显示当前所在的目录地址</p>
<p>/Users/alan/git/learngit</p>
<p>使用</p>
<p>git init 创建一个新的仓库</p>
<p>修改：</p>
<p>git add 文件名（带后缀）</p>
<p>git commit -m “说明” </p>
<p>查看本地仓库状态</p>
<p>git status</p>
<p>查看修改在何处</p>
<p>git diff</p>
<p>查看修改日志</p>
<p>git log</p>
<p>美观可以改为</p>
<p>git log —pretty=oneline</p>
<p>返回上一次修改后的版本（每一个^返回一个版本）（HEAD 表示当前版本）</p>
<p>git reset —hard HEAD^</p>
<p>前进版本(hard 后面是版本号，从log中可以得出)</p>
<p>git reset —hard b3fe8e</p>
<p>找版本号可以</p>
<p>git reflog (命令日志)</p>
<p>从这里可以找到版本号</p>
<p>舍弃工作区的修改（返回add状态的版本或者最新commit的版本）</p>
<p>git checkout — readme.txt </p>
<p>舍弃缓存区的修改</p>
<p>git reset HEAD readme.txt</p>
<p>关联远程仓库(先有本地库再有远程库的时候才需要)</p>
<p>git remote add origin git@github.com:alanhuangym/learngit.git</p>
<p>把本地内容推送到远程仓库上（第一次加-u）</p>
<p>git push -u origin master</p>
<p>git push origin master</p>
<p>从远程库中下载到本地库中(会自动新建文件夹)</p>
<p>git clone git@github.com:alanhuangym/gitskills.git</p>
<p>分支：</p>
<p>查看分支</p>
<p>git branch</p>
<p>创建分支</p>
<p>git branch 分支名</p>
<p>切换分支</p>
<p>git checkout 分支名</p>
<p>在master下面合并分支</p>
<p>git merge 分支名</p>
<p>删除分支</p>
<p>git branch -d 分支名</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Source:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000&quot; target=&quot;_blank&quot; rel=&quot;external
    
    </summary>
    
      <category term="CUHK_Notes" scheme="http://www.pirrla.cn/categories/CUHK-Notes/"/>
    
    
      <category term="git" scheme="http://www.pirrla.cn/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>Coursera Machine Learning Notes</title>
    <link href="http://www.pirrla.cn/2017/03/15/Coursera-MachineLearningByAndrewNg/"/>
    <id>http://www.pirrla.cn/2017/03/15/Coursera-MachineLearningByAndrewNg/</id>
    <published>2017-03-14T16:00:00.000Z</published>
    <updated>2017-03-26T09:34:15.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.coursera.org/learn/machine-learning/home/" target="_blank" rel="external">CourseHomePage</a></p>
<p>Week6:</p>
<p>Evaluating a Learning Algorithm</p>
<p>How to improve the algorithm:</p>
<p>What to do next:</p>
<ol>
<li>Get more training examples</li>
<li>Try smaller or additional features</li>
<li>Try adding ploynomial features (x square or x cube)</li>
<li>Try decrease or increse the lamda (regularization parameter)</li>
</ol>
<p>But every one takes much time. So</p>
<h3 id="Evaluate-the-hypothesis"><a href="#Evaluate-the-hypothesis" class="headerlink" title="Evaluate the hypothesis:"></a>Evaluate the hypothesis:</h3><p>Divide the dataset to traning set : cross validation set : test set = 6:2:2</p>
<p>Tune the parameters on cross validation set. Minimize the error in CV set. </p>
<p>Then only apply test on test set.</p>
<h3 id="Bias-VS-Variance"><a href="#Bias-VS-Variance" class="headerlink" title="Bias VS Variance"></a>Bias VS Variance</h3><p>High means (underfit and overfit)</p>
<ol>
<li>Get more training examples - fix high variance</li>
<li>Try smaller features - fix high variance</li>
<li>Try additional features - fix high bias</li>
<li>Try adding polynomial features - fix high bias</li>
<li>Try decrease lamda - fix high bias</li>
<li>Try increase lamda - fix high variance</li>
</ol>
<p>Precision and Recall</p>
<p>Precision = True Positive/(True positive + False positive) </p>
<p>= True Pos/Number of Predictive Pos</p>
<p>Recall = True positive /(True positive + False negative)</p>
<p>= True Pos/Number of Actual Pos</p>
<p>one way to balance precision and recall is f1 score</p>
<p>f1 score = 2*P*R/(P+R)</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;CourseHomePage&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Week6:&lt;/p&gt;
&lt;p&gt;Ev
    
    </summary>
    
      <category term="CUHK_Notes" scheme="http://www.pirrla.cn/categories/CUHK-Notes/"/>
    
    
      <category term="machine_learning" scheme="http://www.pirrla.cn/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Duality of Linear Programming</title>
    <link href="http://www.pirrla.cn/2017/03/15/Duality_of_Linear_Programming/"/>
    <id>http://www.pirrla.cn/2017/03/15/Duality_of_Linear_Programming/</id>
    <published>2017-03-14T16:00:00.000Z</published>
    <updated>2017-03-25T18:00:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>Resource:</p>
<p><strong>S´ebastien Lahaie, How to take the Dual of a Linear Program <a href="http://www.cs.columbia.edu/coms6998-3/lpprimer.pdf" target="_blank" rel="external">http://www.cs.columbia.edu/coms6998-3/lpprimer.pdf</a></strong></p>
<p>翻译：</p>
<p>Alan Huang</p>
<h1 id="如何求线性规划的二元性"><a href="#如何求线性规划的二元性" class="headerlink" title="如何求线性规划的二元性"></a>如何求线性规划的二元性</h1><ol>
<li><p>线性规划是最优解问题的一个公式化结果：通过设立目标函数和约束函数，求取目标方程的最大值或最小值。因目标函数是线性的，所以称之为线性规划。</p>
<p>通常，线性规划的一般形式是这样的：</p>
<p>max<br>x1≥0, x2≤0, x3<br>v1x1 + v2x2 + v3x3 (1)<br>such that </p>
<p>a1x1 + x2 + x3 ≤ b1 (2)<br>x1 + a2x2 = b2 (3)<br>a3x3 ≥ b3 (4)</p>
<p>(1)是目标函数，(2)(3)(4)是约束函数，目标是求取(1)的最大值</p>
<p>其中，x1,x2,x3是变量，而其他的参数则为常量（v1,v2,v3等）</p>
</li>
<li><p>这里我们称以上所述的线性规划为原始线性规划，二元性则是通过数学过程，从原始线性规划得到二次线性规划。</p>
</li>
<li><p>首先我们要重写原始线性规划的目标方程，使之成为一个标准形式。</p>
<p>为了简便计算，我们尽量将目标方程改写为求##最小值##的形式</p>
<p>从求最大值到求最小值也很简单，只需要将目标方程乘以-1即可</p>
<p>所以我们可以获得改写后的目标方程：</p>
<p>min<br>x1≥0, x2≤0, x3<br>−v1x1 − v2x2 − v3x3</p>
</li>
<li><p>第二步，是将约束函数全部改写为小于或小于等于或等于零的形式。即将(2)(3)(4)改写为：</p>
<p>a1x1 + x2 + x3 − b1 ≤ 0 (5)<br>x1 + a2x2 − b2 = 0 (6)<br>−a3x3 + b3 ≤ 0 (7)</p>
</li>
<li><p>第三步，为每一个不等式约束函数设定一个非负的二元性参数，为每一个等式约束函数设定一个二元性参数（无限制负或非负）。如上所述，我们分别为(5)(7)设定二元性参数λ1 ≥ 0 和 λ3 ≥ 0，为(6)设定 λ2。</p>
</li>
<li><p>第四步，将第三步设置的二元性参数与约束函数符号左边的部分相乘并将它们加入到目标函数之中，并在目标函数前面加上求最大值，即如下所示：</p>
<p>max min  −v1x1 − v2x2 − v3x3</p>
<p>+λ1 (a1x1 + x2 + x3 − b1) (8)</p>
<p>+λ2 (x1 + a2x2 − b2) (9)</p>
<p>+λ3 (−a3x3 + b3) (10)</p>
</li>
<li><p>第五步，将目标函数的括号拆开，并进行整理。将包含原本变量（x1,x2,x3）的式子下放到约束函数之中，其余的项放入目标函数之中。即:</p>
<p>max min  −b1λ1 − b2λ2 − b3λ3</p>
<p>+x1(a1λ1 + λ2 − v1) (11)</p>
<p>+x2(λ1 + a2λ2 − v2) (12)</p>
<p>+x3(λ1 − a3λ3 − v3) (13)</p>
</li>
<li><p>第六步，将约束函数表示为等式或不等式的形式。</p>
<p>如果变量后面的参数是大于或等于零的，则去掉原始变量获得不等式该参数大于或等于零，</p>
<p>如果变量后面的参数是小于或等于零的，则去掉原始变量获得不等式该参数小于或等于零，</p>
<p>如果变量后面的参数是等于零的，则去掉原始变量获得不等式该参数等于零。</p>
<p>同时，目标函数去除Min。</p>
<p>max −b1λ1 − b2λ2 − b3λ3<br>a1λ1 + λ2 − v1 ≥ 0 (14)<br>λ1 + a2λ2 − v2 ≤ 0 (15)<br>λ1 − a3λ3 − v3 = 0 (16)</p>
</li>
<li><p>第七步，如果一开始将求最小值变成了最大值（目标函数乘以-1），则将目标函数再次反转，否则，不做任何步骤（可以整理约束函数），则获得最终结果。</p>
<p>min b1λ1 + b2λ2 + b3λ3<br>a1λ1 + λ2 ≥ v1 (17)<br>λ1 + a2λ2 ≤ v2 (18)<br>λ1 − a3λ3 = v3 (19)</p>
<p>至此，已经获得了原始线性规划的二元线性规划。</p>
<p>​</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Resource:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;S´ebastien Lahaie, How to take the Dual of a Linear Program &lt;a href=&quot;http://www.cs.columbia.edu/coms6998-3/lppri
    
    </summary>
    
      <category term="CUHK_Notes" scheme="http://www.pirrla.cn/categories/CUHK-Notes/"/>
    
    
      <category term="linear_programming" scheme="http://www.pirrla.cn/tags/linear-programming/"/>
    
  </entry>
  
</feed>
